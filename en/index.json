[{"content":"In all my professional experiences so far I\u0026rsquo;ve seen a lot of people/teams ignoring the meaning behind a Pull Request (PR), treating it like a kind of bureaucracy to deploy their piece of code.\nSometimes I heard people saying to each other: \u0026ldquo;I\u0026rsquo;ll open a PR, just accept it because everything is fine\u0026rdquo;. In the end that accepted PR crashed the entire system, I can say that reviewer was as wrong as the reviewee because he was negligent!\nWe can\u0026rsquo;t automate everything, people think by using automated tests they don\u0026rsquo;t have to do anything else to guarantee the code quality.\nBy automated tests I mean:\n Unit Tests, Integration Tests, etc. Static Tests (linters) Static Typing Tests (if you\u0026rsquo;re using a language with optional static type like Python)  In the next topics, I\u0026rsquo;ll try to explain my point of view about code and its quality in the business world!\nWhat is the code?  Every project/codebase is a company\u0026rsquo;s asset.\n The code we write isn\u0026rsquo;t ours, the code belongs to the company. We\u0026rsquo;re just momentarily responsible for it, perhaps in a few weeks the project goes to another team, and also another project can reach our team!\nDue to the fact that projects can be moved around teams, it\u0026rsquo;s our duty to maintain the quality of the code we write, we need to care about it not because someone wants but because we should, it\u0026rsquo;s a task for everyone who writes code.\nQuality  Quality is an empathy thing with who maintained, maintains and will maintain the code.\n Many people think that automated tests are enough to guarantee quality but the equation should be something like this:\n quality = automated tests + legibility + maintainability + good architecture\n Why does the review process exist in the Pull Request if I can automatically merge it after a successful pipeline?\nBecause we can\u0026rsquo;t guarantee legibility, maintainability, and good architecture. People think that running a linter and having 100% code coverage is enough but it isn\u0026rsquo;t.\nRemember, code coverage doesn\u0026rsquo;t mean test quality!\nAs a team, we\u0026rsquo;re responsible for the code in the main branch. When someone creates other branches to implement a new feature or fix some bug who created the branch and the reviewer are responsible to guarantee the quality of the code that will be merged into the main branch.\nSome people say that code is just a way to reach the product, but if that code doesn\u0026rsquo;t have quality and definition, what will this mean to the final product?\nProbably a product that no one will want to maintain because it\u0026rsquo;ll be hard to understand how it works.\nPull Request A Pull Request is a request to add/change the company\u0026rsquo;s asset, it\u0026rsquo;s something that can be in production already, so, we need to guarantee high quality because our customers will be directly affected.\nThe Pull Request is where we can validate every quality aspect of the code that will be merged into the main branch, trying to reduce any possible mistakes in the code!\nOne of the most exciting things about Pull Requests is that everyone learns something from both sides (reviewer and reviewee).\n Special Thanks to people who reviewed:\n Nikita Sobolevn Gustavo Millen Bruno Delfino  Already published on:\n Medium dev.to  ","permalink":"https://thepabloaguilar.dev/en/its-not-just-a-pull-request/","summary":"In all my professional experiences so far I\u0026rsquo;ve seen a lot of people/teams ignoring the meaning behind a Pull Request (PR), treating it like a kind of bureaucracy to deploy their piece of code.\nSometimes I heard people saying to each other: \u0026ldquo;I\u0026rsquo;ll open a PR, just accept it because everything is fine\u0026rdquo;. In the end that accepted PR crashed the entire system, I can say that reviewer was as wrong as the reviewee because he was negligent!","title":"It's not just a Pull Request"},{"content":"History A few months ago I\u0026rsquo;ve started to contribute with returns, an amazing Open Source Python library with a lot of containers to help us in many ways, I won\u0026rsquo;t cover those containers here but you can access the documentation page to know more about them.\nHere I\u0026rsquo;m going to cover why we chose the monkey patching technique to implements a feature, Improve Failure Traceability, to the Result containers.\nJust a brief explanation of what is a Result container, basically your code can take two ways:\n Success, the function computation was successfully executed Failure, the function computation broke due to business logic or an Exception was raised  That container abstracts those ways for us, see the example below:\nfrom returns.result import Failure, Result, Success def is_even(number: int) -\u0026gt; Result[int, int]: if arg % 2 == 0: return Success(number) return Failure(number) assert is_even(2) == Success(2) assert is_even(1) == Failure(1) Using Result can be a great idea because you don\u0026rsquo;t have more to deal with raised Business Exceptions and put try...except everywhere in our code, we just have to return a Failure container.\nTracing Failure: feature explanation Failure is great, but Exceptions give to us an important thing: where it was raised.\nInspired by Tracing Failure feature from dry-rb, which is a collection of Ruby Libraries, we started the discussion to give this option to the returns users and one important thing was considered to implement it, users that will not use this feature can\u0026rsquo;t have their system/application performance affected!\nThe simple, easiest and unique (I guess) way to implement the feature is to get the call stack and make some manipulation with it. In Python is simple to get the call stack, but it\u0026rsquo;s a heavy operation that can affect the performance if it\u0026rsquo;s often called. Below you can see extracted metrics about memory consumption when creating Failure container getting the call stack and don\u0026rsquo;t getting it, respectively:\nHow can we implement the tracing feature? We already know the tracing has to be optional because users that won\u0026rsquo;t use it can\u0026rsquo;t be affected, and as we can see when the trace is enabled we have a huge memory overhead!\nTo get the trace optional we had two options:\n Use an environment variable Use monkey patching  By the title of this article you know might we have chosen the second option.\nWhy monkey patching?\nMonkey Patching is a more sophisticated and elegant approach than an environment variable, we can separate in the right way the tracing feature code from the class we want to be traced and we don\u0026rsquo;t depend on an external resource. Using an env variable will end up with something like this in our classes, we can decouple the if statement from the class but in somewhere of our code the if will be there:\nimport os class Example: def __init__(self) -\u0026gt; None: if os.getenv(\u0026#39;RETURNS_TRACE\u0026#39;): self._tracking = [] Monkey patching is a known friend of Python programmers, we use it a lot while writing tests to mock everything we want (e.g. API request, database interaction), but it\u0026rsquo;s not used too much in our \u0026ldquo;production\u0026rdquo; code because have some drawbacks like it\u0026rsquo;s not Thread Safe and can create many bugs since it can affect our entire code base at runtime. But we understood that tracing feature is to development purposes, we don\u0026rsquo;t care about thread safety problem and we know exactly we are monkey patching!\n Can we turn the monkey patching technique thread safe in Python?\nYes, we can but it\u0026rsquo;s a subject for another article.\n After some discussions, we finally delivered our Tracing Failures feature, and now users can active explicit in their code the tracing for the Result containers.\nfrom returns.result import Failure, Result from returns.primitives.tracing import collect_traces @collect_traces def get_failure(argument: str) -\u0026gt; Result[str, str]: return Failure(argument) failure = get_failure(\u0026#39;example\u0026#39;) for trace_line in failure.trace: print(f\u0026#34;{trace_line.filename}:{trace_line.lineno} in `{trace_line.function}`\u0026#34;) The output should be something like:\n/returns/returns/result.py:529 in `Failure` /example_folder/example.py:5 in `get_failure` /example_folder/example.py:1 in `\u0026lt;module\u0026gt;` Extra The main goal of the tracing feature is to give to the user the ability to find where the failure occurred, but if you don\u0026rsquo;t want to analyze the call stack and know the scenario where the failure occurs use returns pytest plugin to verify your hypothesis. We provide a fixture called returns with has_trace method, see the example below:\nfrom returns.result import Result, Success, Failure def example_function(arg: str) -\u0026gt; Result[int, str]: if arg.isnumeric(): return Success(int(arg)) return Failure(\u0026#39;\u0026#34;{0}\u0026#34; is not a number\u0026#39;.format(arg)) def test_if_failure_is_created_at_example_function(returns): with returns.has_trace(Failure, example_function): Success(\u0026#39;not a number\u0026#39;).bind(example_function) If test_if_failure_is_created_at_example_functions fails we know the failure is not created at example_function or in its internal calls.\nRelated links  Hear no evil, see no evil, patch no evil: Or, how to monkey-patch safely. Monkey Patching and its consequences   Already published on:\n Medium dev.to  ","permalink":"https://thepabloaguilar.dev/en/monkey-patching-is-not-too-bad/","summary":"History A few months ago I\u0026rsquo;ve started to contribute with returns, an amazing Open Source Python library with a lot of containers to help us in many ways, I won\u0026rsquo;t cover those containers here but you can access the documentation page to know more about them.\nHere I\u0026rsquo;m going to cover why we chose the monkey patching technique to implements a feature, Improve Failure Traceability, to the Result containers.\nJust a brief explanation of what is a Result container, basically your code can take two ways:","title":"Monkey Patching is not too bad"}]